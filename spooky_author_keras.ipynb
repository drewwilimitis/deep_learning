{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:30:31.704699Z",
     "start_time": "2019-02-18T14:28:49.440329Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# set options for rendering plots\n",
    "%matplotlib inline\n",
    "\n",
    "# display multiple outputs within a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\";\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore');\n",
    "\n",
    "train = pd.read_csv(\"C:/Users/dreww/Documents/train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/dreww/Documents/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:31:17.364811Z",
     "start_time": "2019-02-18T14:31:17.357996Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:31:22.264460Z",
     "start_time": "2019-02-18T14:31:22.131451Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:31:25.907230Z",
     "start_time": "2019-02-18T14:31:23.531727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "text = remove_stopwords(train[\"text\"])\n",
    "test_txt = remove_stopwords(test[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:31:35.269082Z",
     "start_time": "2019-02-18T14:31:33.823770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(19579,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24882 unique tokens.\n",
      "Shape of data tensor: (19579, 200)\n",
      "Shape of label tensor: (19579,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(text)\n",
    "data.shape\n",
    "labels = np.array(train[\"author\"])\n",
    "labels.shape\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#train_text = remove_stopwords(train['text'])\n",
    "#test_text = remove_stopwords(test['text'])\n",
    "\n",
    "# keep only maxlen words per sentence\n",
    "maxlen = 200\n",
    "\n",
    "# take cutoff of most common words\n",
    "max_words = 20000\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:31:38.491297Z",
     "start_time": "2019-02-18T14:31:38.478534Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert labels to floats\n",
    "labels[labels == 'EAP'] = 0\n",
    "labels[labels == 'HPL'] = 1\n",
    "labels[labels == 'MWS'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T02:23:23.853737Z",
     "start_time": "2019-02-18T02:22:29.081611Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 7s 448us/step - loss: 1.0731 - acc: 0.4182 - val_loss: 1.0371 - val_acc: 0.4778\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 5s 327us/step - loss: 0.9290 - acc: 0.5716 - val_loss: 0.8684 - val_acc: 0.6315\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 5s 329us/step - loss: 0.7110 - acc: 0.7139 - val_loss: 0.6888 - val_acc: 0.7173\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 5s 324us/step - loss: 0.5308 - acc: 0.7984 - val_loss: 0.6054 - val_acc: 0.7574\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 5s 320us/step - loss: 0.4140 - acc: 0.8461 - val_loss: 0.5407 - val_acc: 0.7914\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 5s 347us/step - loss: 0.3355 - acc: 0.8761 - val_loss: 0.5779 - val_acc: 0.7699\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 5s 320us/step - loss: 0.2799 - acc: 0.8980 - val_loss: 0.5481 - val_acc: 0.7937\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 5s 322us/step - loss: 0.2347 - acc: 0.9161 - val_loss: 0.5611 - val_acc: 0.7870\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 6s 378us/step - loss: 0.2043 - acc: 0.9262 - val_loss: 0.5786 - val_acc: 0.7909\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 5s 329us/step - loss: 0.1727 - acc: 0.9390 - val_loss: 0.5907 - val_acc: 0.7888\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 32))\n",
    "model.add(SimpleRNN(32, dropout=0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(data, labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:35:04.108732Z",
     "start_time": "2019-02-18T14:31:56.660704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 10s 663us/step - loss: 1.0884 - acc: 0.4016 - val_loss: 1.0866 - val_acc: 0.3999\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 9s 579us/step - loss: 1.0846 - acc: 0.4044 - val_loss: 1.0840 - val_acc: 0.3999\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 10s 608us/step - loss: 1.0796 - acc: 0.4044 - val_loss: 1.0758 - val_acc: 0.4002\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 9s 584us/step - loss: 1.0635 - acc: 0.4054 - val_loss: 1.0525 - val_acc: 0.4027\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 10s 608us/step - loss: 1.0174 - acc: 0.4618 - val_loss: 0.9875 - val_acc: 0.4487\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 9s 597us/step - loss: 0.9164 - acc: 0.6300 - val_loss: 0.8659 - val_acc: 0.6404\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 9s 579us/step - loss: 0.7824 - acc: 0.7305 - val_loss: 0.7460 - val_acc: 0.7699\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 9s 600us/step - loss: 0.6689 - acc: 0.7721 - val_loss: 0.6500 - val_acc: 0.8039\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 9s 573us/step - loss: 0.5736 - acc: 0.8087 - val_loss: 0.5818 - val_acc: 0.7903\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 9s 592us/step - loss: 0.5055 - acc: 0.8235 - val_loss: 0.5321 - val_acc: 0.8166\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 9s 573us/step - loss: 0.4454 - acc: 0.8495 - val_loss: 0.5079 - val_acc: 0.8008\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 9s 594us/step - loss: 0.4036 - acc: 0.8641 - val_loss: 0.4777 - val_acc: 0.8238\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 9s 597us/step - loss: 0.3723 - acc: 0.8724 - val_loss: 0.4601 - val_acc: 0.8235\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 9s 585us/step - loss: 0.3426 - acc: 0.8835 - val_loss: 0.4432 - val_acc: 0.8271\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 9s 598us/step - loss: 0.3164 - acc: 0.8925 - val_loss: 0.4347 - val_acc: 0.8284\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 9s 577us/step - loss: 0.2977 - acc: 0.8981 - val_loss: 0.4254 - val_acc: 0.8322\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 9s 598us/step - loss: 0.2762 - acc: 0.9072 - val_loss: 0.4243 - val_acc: 0.8343\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 9s 589us/step - loss: 0.2624 - acc: 0.9126 - val_loss: 0.4188 - val_acc: 0.8325\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 9s 590us/step - loss: 0.2474 - acc: 0.9146 - val_loss: 0.4205 - val_acc: 0.8343\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 9s 590us/step - loss: 0.2319 - acc: 0.9207 - val_loss: 0.4259 - val_acc: 0.8309\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 36))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(data, \n",
    "                    labels,\n",
    "                    epochs=25,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T14:35:18.725340Z",
     "start_time": "2019-02-18T14:35:17.390677Z"
    }
   },
   "outputs": [],
   "source": [
    "# make test predictions\n",
    "test_txt = tokenizer.texts_to_sequences(test_txt)\n",
    "X_test = pad_sequences(sequences=test_txt, maxlen=maxlen)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=16)\n",
    "test['EAP'] = predictions[:, 0]\n",
    "test['HPL'] = predictions[:, 1]\n",
    "test['MWS'] = predictions[:, 2]\n",
    "\n",
    "# final submission\n",
    "test[['id', 'EAP', 'HPL', 'MWS']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
